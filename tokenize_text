#import modules 
import nltk
nltk.download("punkt")
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

# load and read text
print ("Load and read text...")
OpenFile = open("constitution_of_usa.txt")
ReadText = OpenFile.read()

# remove new lines and tabs
print ("Remove new lines and tabs ...")
RawText = ReadText.replace("\n", " ").replace('"', " ").replace(")", " ").replace("(", " ").replace("\t", " ").replace("-", " ").replace("  ", " ")

# Tokenization
print ("Tokenizing text into sentences ...")
TokenizeSent = sent_tokenize(RawText)

print ("Tokenizing sentences into words ...")

TokenizeWords = word_tokenize(RawText)

print (TokenizeWords)

# select a paragraph 
select_para = TokenizeSent[1]
print(select_para)

# select a word 
select_word = TokenizeWords[1]
print(select_word)
